{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def sum(csv_file):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, header=0, index_col=0)\n",
    "        # Exclude first column (variable names)\n",
    "        matrix_data = df.iloc[:, 1:].to_numpy()\n",
    "        # Calculate sum of nonzeros\n",
    "        sum = matrix_data.sum()\n",
    "        return sum\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/naufaamirani/Documents/Columbia/CBMFW4761/deeptfni_colab/Input_data'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        sum = sum(file_path)\n",
    "        print(f\"Sum of 1's in {file_name}: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def process_adj(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, index_col=None)\n",
    "    # Assuming the first row contains column names\n",
    "    df.columns = df.iloc[0]  \n",
    "    # Reset index after setting columns\n",
    "    df = df.iloc[1:].reset_index(drop=True) \n",
    "    # df = df.drop(['TF',axis=1])\n",
    "    df = df.astype(int, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def confusion(y_true_df, y_pred_df):\n",
    "    # Flatten dfs for metrics\n",
    "    y_true_flat = y_true_df.values.flatten()\n",
    "    y_pred_flat = y_pred_df.values.flatten()\n",
    "    # Convert string to numeric\n",
    "    y_true_numeric = np.where(y_true_flat == '1', 1, 0).astype(int)\n",
    "    y_pred_numeric = np.where(y_pred_flat == '1', 1, 0).astype(int)\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true_numeric, y_pred_numeric)\n",
    "    # Compute accuracy precision recall\n",
    "    accuracy = accuracy_score(y_true_numeric, y_pred_numeric)\n",
    "    precision = precision_score(y_true_numeric, y_pred_numeric, average='binary')\n",
    "    recall = recall_score(y_true_numeric, y_pred_numeric, average='binary')\n",
    "\n",
    "    # Find indices where true label= 1 and predicted label= 1\n",
    "    true_positive_indices = np.where((y_true_numeric == 1) & (y_pred_numeric == 1))[0]\n",
    "\n",
    "    if len(true_positive_indices) > 0:\n",
    "        true_positive_columns = y_true_df.columns.tolist()\n",
    "        # Filter true positive indices to ensure they are within the range of column names\n",
    "        true_positive_indices_filtered = [idx for idx in true_positive_indices if idx < len(true_positive_columns)]\n",
    "        # Get column names\n",
    "        true_positive_column_names = [true_positive_columns[idx] for idx in true_positive_indices_filtered]\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                    yticklabels=['Actual 0', 'Actual 1'])\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        return accuracy, precision, recall, true_positive_indices_filtered\n",
    "    else:\n",
    "        print(\"No true positives found.\")\n",
    "        return accuracy, precision, recall, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pred_dir = \"/Users/naufaamirani/Downloads/results/final adjacency matrix/binary\"\n",
    "true_dir = \"/Users/naufaamirani/Downloads/results/initial adjacency matrix\"\n",
    "\n",
    "names = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "tp_indices = []\n",
    "\n",
    "for file in os.listdir(pred_dir):\n",
    "    print(file+'\\n')\n",
    "    y_pred = process_adj(pred_dir + os.sep + file)\n",
    "    sample = \"_\".join(file.split(\"_\")[0:4])\n",
    "    print(sample+'\\n')\n",
    "    # COMBO & GENE use the same initial adjacency\n",
    "    if sample.startswith(\"COMBO\"):\n",
    "        temp_sample = sample.replace(\"COMBO\", \"GENE\")\n",
    "        y_true_path = true_dir + os.sep + temp_sample + \"_subset.txt\"  \n",
    "    # SOLO & RNASEQ use the same initial adjacency\n",
    "    elif sample.startswith(\"RNASEQ\"):\n",
    "        temp_sample = sample.replace(\"RNASEQ\", \"SOLO\")\n",
    "        y_true_path = true_dir + os.sep + temp_sample + \"_subset.txt\"  \n",
    "    else:\n",
    "        y_true_path = true_dir + os.sep + sample + \"_subset.txt\"\n",
    "\n",
    "    print(y_true_path)\n",
    "    y_true = process_adj(y_true_path)\n",
    "\n",
    "    # Get scores\n",
    "    accuracy, precision, recall, tp_i = confusion(y_true, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    names.append(sample)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    tp_indices.append(len(tp_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_third_token(names, accuracies):\n",
    "    def get_third_token(filename):\n",
    "        return filename.split('_')[3]\n",
    "\n",
    "    unique_third_tokens = sorted(set(get_third_token(filename) for filename in names))\n",
    "    # Split filenames and accuracies into lists based on third token\n",
    "    data_by_third_token = {token: {'filenames': [], 'accuracies': []} for token in unique_third_tokens}\n",
    "    for filename, accuracy in zip(names, accuracies):\n",
    "        third_token = get_third_token(filename)\n",
    "        data_by_third_token[third_token]['filenames'].append(filename)\n",
    "        data_by_third_token[third_token]['accuracies'].append(accuracy)\n",
    "\n",
    "    # Convert dict to lists of filenames and accuracies\n",
    "    names_lists = []\n",
    "    accuracies_lists = []\n",
    "\n",
    "    for token_data in data_by_third_token.values():\n",
    "        names_lists.append(token_data['filenames'])\n",
    "        accuracies_lists.append(token_data['accuracies'])\n",
    "\n",
    "    return names_lists, accuracies_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_lists, accuracies_lists = split_data_by_third_token(names, accuracies)\n",
    "print(len(names_lists))\n",
    "print(names_lists[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_lists, precisions_lists = split_data_by_third_token(names, precisions)\n",
    "print(len(precisions_lists))\n",
    "print(precisions_lists[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_lists, precisions_lists = split_data_by_third_token(names, recalls)\n",
    "print(len(recalls_lists))\n",
    "print(recalls_lists[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(nams, accs, name):\n",
    "    # Nested dict to store aggregated values by (category, subcategory) pairs\n",
    "    category_subcategory_values = {}\n",
    "    # Process each name and corresponding value\n",
    "    for filename, value in zip(nams, accs):\n",
    "        tokens = filename.split('_')\n",
    "        category = tokens[0]\n",
    "        subcategory = tokens[1]\n",
    "        \n",
    "        # Update the dictionary with the (category, subcategory) pair and its associated value\n",
    "        # category = COMBO/GENE/RNASEQ/SOLO\n",
    "        # subcategory = ALS/CTR\n",
    "        if (category, subcategory) in category_subcategory_values:\n",
    "            category_subcategory_values[(category, subcategory)].append(value)\n",
    "        else:\n",
    "            category_subcategory_values[(category, subcategory)] = [value]\n",
    "\n",
    "    # Calculate averages for each (category, subcategory)\n",
    "    averages = {}\n",
    "    for key, value_list in category_subcategory_values.items():\n",
    "        avg = np.mean(value_list)*100\n",
    "        rounded_avg = round(avg, 2)\n",
    "        percentage_avg = \"{:.2f}%\".format(rounded_avg)\n",
    "        averages[key] = percentage_avg\n",
    "        print(str(key) + \": \" + str(percentage_avg))\n",
    "\n",
    "    categories = sorted(set(category for category, subcategory in averages.keys()))\n",
    "    subcategories = sorted(set(subcategory for category, subcategory in averages.keys()))\n",
    "\n",
    "    # Prepare for plotting multiple bars\n",
    "    # Average values grouped by category and subcategory)\n",
    "    data = [[averages[(category, subcategory)] for subcategory in subcategories] for category in categories]\n",
    "    num_subcategories = len(subcategories)\n",
    "\n",
    "    bar_width = 0.1\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    for i, category in enumerate(categories):\n",
    "        x = np.arange(num_subcategories) + i * bar_width\n",
    "        ax.bar(x, data[i], width=bar_width, label=category)\n",
    "    ax.set_xlabel('Subcategory')\n",
    "    ax.set_ylabel('Average Values')\n",
    "    ax.set_title('Count of False Positives by Model Types ('+name+')')\n",
    "    ax.set_xticks(np.arange(num_subcategories) + (len(categories) - 1) * bar_width / 2)\n",
    "    ax.set_xticklabels(subcategories)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), shadow=True, ncol=len(categories))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(names_lists[0], recalls_lists[0], \"Astro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(names_lists[1], recalls_lists[1], \"Exc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
